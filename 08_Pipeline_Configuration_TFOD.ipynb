{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbpdVKR4Cst-"
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Pipeline Configuration in TFOD </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-GbTlRrDNoX"
   },
   "source": [
    "Every model present in **Model Zoo** of **TensorFlow Object Detection API** comes with a configuration file called `pipeline.config` file. It let's us modify different parameters of the model like number of classes, batch size etc. We will also need to edit this file to add paths to our training dataset and other required files.\n",
    "\n",
    "<img src = \"https://opencv.org/wp-content/uploads/2022/07/c4_06_pipeline_config.png\" align='center' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNWUB_ZFuwn1"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [1 Install the TFOD API](#1-Install-the-TFOD-API)\n",
    "* [2 Download the Model](#2-Download-the-Model)\n",
    "* [3 Important Config Parameters](#3-Important-Config-Parameters)\n",
    "* [4 Label Map](#4-Label-Map)\n",
    "* [5 Additional Params](#5-Additional-Params)\n",
    "* [6 Save the edited Config File](#6-Save-the-edited-Config-File)\n",
    "* [7 Conclusion](#7-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn3ZPB-6PVLV"
   },
   "source": [
    "## 1 Install the TFOD API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5_CMahSQFqu"
   },
   "source": [
    "**Clone the GitHub Repo**\n",
    "\n",
    "Here we will clone the TFOD API repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjXAzLgrP9qz",
    "outputId": "f1c8fbed-fa04-4743-989f-7899d2b419ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 3451, done.\u001b[K\n",
      "remote: Counting objects: 100% (3451/3451), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2891/2891), done.\u001b[K\n",
      "remote: Total 3451 (delta 891), reused 1416 (delta 503), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (3451/3451), 46.85 MiB | 32.20 MiB/s, done.\n",
      "Resolving deltas: 100% (891/891), done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not 'models' in os.listdir():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_p-yWY1uQAPC",
    "outputId": "c7ec95e3-8a96-4bdb-f624-c409dd7d96dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Processing /content/models/research\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
      "Collecting apache-beam\n",
      "  Downloading apache_beam-2.40.0-cp37-cp37m-manylinux2010_x86_64.whl (10.9 MB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.9.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.32)\n",
      "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
      "Collecting tf-slim\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.4)\n",
      "Collecting lvis\n",
      "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.7.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.3.5)\n",
      "Collecting tf-models-official>=2.5.1\n",
      "  Downloading tf_models_official-2.9.2-py2.py3-none-any.whl (2.1 MB)\n",
      "Collecting tensorflow_io\n",
      "  Downloading tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.9.0)\n",
      "Collecting pyparsing==2.4.7\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.21.6)\n",
      "Requirement already satisfied: tensorflow~=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.9.1)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting tensorflow-text~=2.9.0\n",
      "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.6.0.66)\n",
      "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.11)\n",
      "Collecting pyyaml<6.0,>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
      "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
      "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.6.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
      "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.31.6)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.56.4)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2022.2)\n",
      "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2022.6.15)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.26.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.47.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (2.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (2.9.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (14.0.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.1)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.8.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.7)\n",
      "Collecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.22.0-py3-none-any.whl (47 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
      "Collecting fastavro<2,>=0.23.6\n",
      "  Downloading fastavro-1.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "Collecting cloudpickle<3,>=2.1.0\n",
      "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (6.0.1)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.7.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (275 kB)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting protobuf<4.0.0dev,>=3.12.0\n",
      "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.1.0)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.6.0.66)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.10)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2022.6.2)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.7.1)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.9.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.9.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n",
      "Building wheels for collected packages: object-detection, py-cpuinfo, dill, avro-python3, docopt, seqeval\n",
      "  Building wheel for object-detection (setup.py): started\n",
      "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
      "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1694955 sha256=55f5f60c93c977e676f81ac53806df217b61f76e9ebf7a01205b8d60d356cb93\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-daq6g6rf/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n",
      "  Building wheel for py-cpuinfo (setup.py): started\n",
      "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=f334fc20ab536d37b59f514595527d81b9b1c77417cd601f35358f3e8cb34b60\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=76ebc4174b5bb2a9d0c75345499e7f89ed9066ebd4f2bbffb8732a79a0d970d6\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
      "  Building wheel for avro-python3 (setup.py): started\n",
      "  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
      "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44010 sha256=f0e6afb2cca6816dc8735a2efda25be57989b95c08c51b63ebd5c4cbd051279e\n",
      "  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=ee20694fbecf8d1c056533144c029196d15b2e6591e6ae0abb99050d23decb0f\n",
      "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=1cdbc5be8a423a5ceea8a1468829aad89c626d138459ac6118021a970d8ace44\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "Successfully built object-detection py-cpuinfo dill avro-python3 docopt seqeval\n",
      "Installing collected packages: requests, pyparsing, protobuf, portalocker, docopt, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, py-cpuinfo, proto-plus, orjson, hdfs, fastavro, cloudpickle, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.9\n",
      "    Uninstalling pyparsing-3.0.9:\n",
      "      Successfully uninstalled pyparsing-3.0.9\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.17.3\n",
      "    Uninstalling protobuf-3.17.3:\n",
      "      Successfully uninstalled protobuf-3.17.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Attempting uninstall: pymongo\n",
      "    Found existing installation: pymongo 4.2.0\n",
      "    Uninstalling pymongo-4.2.0:\n",
      "      Successfully uninstalled pymongo-4.2.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.3.0\n",
      "    Uninstalling cloudpickle-1.3.0:\n",
      "      Successfully uninstalled cloudpickle-1.3.0\n",
      "Successfully installed apache-beam-2.40.0 avro-python3-1.10.2 cloudpickle-2.1.0 colorama-0.4.5 dill-0.3.1.1 docopt-0.6.2 fastavro-1.6.0 hdfs-2.7.0 lvis-0.5.3 object-detection-0.1 orjson-3.7.12 portalocker-2.5.1 proto-plus-1.22.0 protobuf-3.19.4 py-cpuinfo-8.0.0 pymongo-3.12.3 pyparsing-2.4.7 pyyaml-5.4.1 requests-2.28.1 sacrebleu-2.2.0 sentencepiece-0.1.97 seqeval-1.2.2 tensorflow-addons-0.17.1 tensorflow-io-0.26.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.9.0 tf-models-official-2.9.2 tf-slim-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# The `%%bash` magic command inside a notebook lets you run a cell run like a shell interface\n",
    "# Note: the `bash` command works only on Colab.\n",
    "%%bash \n",
    "\n",
    "# Change the directory to models/research\n",
    "cd models/research/\n",
    "\n",
    "# Compile the API's Protobuf files\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "# Copy the required Setup file\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "\n",
    "# Install the API using the setup.py file\n",
    "python -m pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-AV5FtDuwn2"
   },
   "source": [
    "In this notebook we will go over different paramaters that can be modified in this file. We will also provide a description of each parameter and also share code to modify those parameters. \n",
    "\n",
    "We should note that the `pipeline.config` file enables us to modify almost all the parameters in the file; but we shall restrict ourselves to only those parameters that would be required for fine tuning a pretrained model. \n",
    "\n",
    "Now you can open up the `pipeline.config` file in a text editor and change each value, but it is always not a wise solution to update the `config` file manually; however, **TFOD API** provides us with a convinient module called `config_util`. This convenient module allows us to pragramitically change all the required params.\n",
    "\n",
    "We will start by importing the `config_util` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8P_pEox5uwn2"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "from object_detection.utils import config_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGs6z836uwn4"
   },
   "source": [
    "## 2 Download the Model\n",
    "\n",
    "First, let us download the Model from the official [TensorFlow Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) repo. You can use any model you like, we will be using a pre-trained **`RetinaNet_101`** model, which will be further used for fine-tuning.\n",
    " \n",
    "This download model function downloads any model from the zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5E-GI4huwn5"
   },
   "outputs": [],
   "source": [
    "# Function to Download any model from Model Zoo from their URL\n",
    "def download_model(model_name, url):\n",
    "\n",
    "    file = requests.get(url)\n",
    "    open(model_name+'.tar.gz', 'wb').write(file.content)\n",
    "\n",
    "    # Extract the Model\n",
    "    tar = tarfile.open(model_name + '.tar.gz')\n",
    "    tar.extractall(model_name)\n",
    "    tar.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ5uP8JUuwn6"
   },
   "source": [
    "Download the **`RetinaNet_101`** model by calling the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cU2zY0f3uwn6"
   },
   "outputs": [],
   "source": [
    "# Define URL and name of the Model\n",
    "model_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz'\n",
    "# Directory where the model and its configuration should be downloaded.\n",
    "model_directory = 'RetinaNet_101'\n",
    "\n",
    "model_name = model_url.split('/')[-1].split('.')[0] # ssd_resnet101_v1_fpn_640x640_coco17_tpu-8\n",
    "\n",
    "# Download RetinaNet 101 model from the Model Zoo\n",
    "download_model(model_directory, model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYzmDKeewYkg"
   },
   "source": [
    "After that you'll need to load the `pipeline.config` file with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JilKlkeCcNp"
   },
   "outputs": [],
   "source": [
    "# Define the path where pipeline config is present.\n",
    "base_config_path = os.path.join(model_directory, model_name, 'pipeline.config')\n",
    "\n",
    "# Read the config file in the form a dictionary\n",
    "configs = config_util.get_configs_from_pipeline_file(base_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-9aJ9Xruwn8"
   },
   "source": [
    "## 3 Important Config Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3p4SgpsQMXQ"
   },
   "source": [
    "In this notebook, we have used `ssd_resnet101_v1_fpn_640x640_coco17_tpu-8` as our baseline model. However, you can have a look at the config files of other models present in the model zoo of TFOD API.\n",
    "\n",
    "Basically, the config files of almost all the object detection models in TFOD share the same file structure containing the following keys:\n",
    "\n",
    "- **`model:`** Contains information about the model such as model architecture, loss functions used, post-processing info like NMS, etc\n",
    "\n",
    "\n",
    "- **`train_config:`** Contains information about the train data such as batch size, number of steps to train for, optimizer used, data augmentations used, etc\n",
    "\n",
    "\n",
    "- **`train_input_reader:`** Contains paths to label map and tf record for the training data\n",
    "\n",
    "\n",
    "- **`eval_config:`** Contains information about the evaluation metric to be used\n",
    "\n",
    "\n",
    "- **`eval_input_reader:`** Contains paths to label map and tf record for the validation data\n",
    "\n",
    "\n",
    "For a clearer understanding, let us have a quick look about the model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvjtgdD9QMXR",
    "outputId": "d130ce7b-1edd-4ceb-ff4a-d1df50b35c5d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd {\n",
      "  num_classes: 90\n",
      "  image_resizer {\n",
      "    fixed_shape_resizer {\n",
      "      height: 640\n",
      "      width: 640\n",
      "    }\n",
      "  }\n",
      "  feature_extractor {\n",
      "    type: \"ssd_resnet101_v1_fpn_keras\"\n",
      "    depth_multiplier: 1.0\n",
      "    min_depth: 16\n",
      "    conv_hyperparams {\n",
      "      regularizer {\n",
      "        l2_regularizer {\n",
      "          weight: 0.00039999998989515007\n",
      "        }\n",
      "      }\n",
      "      initializer {\n",
      "        truncated_normal_initializer {\n",
      "          mean: 0.0\n",
      "          stddev: 0.029999999329447746\n",
      "        }\n",
      "      }\n",
      "      activation: RELU_6\n",
      "      batch_norm {\n",
      "        decay: 0.996999979019165\n",
      "        scale: true\n",
      "        epsilon: 0.0010000000474974513\n",
      "      }\n",
      "    }\n",
      "    override_base_feature_extractor_hyperparams: true\n",
      "    fpn {\n",
      "      min_level: 3\n",
      "      max_level: 7\n",
      "    }\n",
      "  }\n",
      "  box_coder {\n",
      "    faster_rcnn_box_coder {\n",
      "      y_scale: 10.0\n",
      "      x_scale: 10.0\n",
      "      height_scale: 5.0\n",
      "      width_scale: 5.0\n",
      "    }\n",
      "  }\n",
      "  matcher {\n",
      "    argmax_matcher {\n",
      "      matched_threshold: 0.5\n",
      "      unmatched_threshold: 0.5\n",
      "      ignore_thresholds: false\n",
      "      negatives_lower_than_unmatched: true\n",
      "      force_match_for_each_row: true\n",
      "      use_matmul_gather: true\n",
      "    }\n",
      "  }\n",
      "  similarity_calculator {\n",
      "    iou_similarity {\n",
      "    }\n",
      "  }\n",
      "  box_predictor {\n",
      "    weight_shared_convolutional_box_predictor {\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 0.00039999998989515007\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          random_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.009999999776482582\n",
      "          }\n",
      "        }\n",
      "        activation: RELU_6\n",
      "        batch_norm {\n",
      "          decay: 0.996999979019165\n",
      "          scale: true\n",
      "          epsilon: 0.0010000000474974513\n",
      "        }\n",
      "      }\n",
      "      depth: 256\n",
      "      num_layers_before_predictor: 4\n",
      "      kernel_size: 3\n",
      "      class_prediction_bias_init: -4.599999904632568\n",
      "    }\n",
      "  }\n",
      "  anchor_generator {\n",
      "    multiscale_anchor_generator {\n",
      "      min_level: 3\n",
      "      max_level: 7\n",
      "      anchor_scale: 4.0\n",
      "      aspect_ratios: 1.0\n",
      "      aspect_ratios: 2.0\n",
      "      aspect_ratios: 0.5\n",
      "      scales_per_octave: 2\n",
      "    }\n",
      "  }\n",
      "  post_processing {\n",
      "    batch_non_max_suppression {\n",
      "      score_threshold: 9.99999993922529e-09\n",
      "      iou_threshold: 0.6000000238418579\n",
      "      max_detections_per_class: 100\n",
      "      max_total_detections: 100\n",
      "      use_static_shapes: false\n",
      "    }\n",
      "    score_converter: SIGMOID\n",
      "  }\n",
      "  normalize_loss_by_num_matches: true\n",
      "  loss {\n",
      "    localization_loss {\n",
      "      weighted_smooth_l1 {\n",
      "      }\n",
      "    }\n",
      "    classification_loss {\n",
      "      weighted_sigmoid_focal {\n",
      "        gamma: 2.0\n",
      "        alpha: 0.25\n",
      "      }\n",
      "    }\n",
      "    classification_weight: 1.0\n",
      "    localization_weight: 1.0\n",
      "  }\n",
      "  encode_background_as_zeros: true\n",
      "  normalize_loc_loss_by_codesize: true\n",
      "  inplace_batchnorm_update: true\n",
      "  freeze_batchnorm: false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(configs['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYCFwsPcDWto"
   },
   "source": [
    "Now let’s take a look at some basic and the most important configurations required for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6cIGlUpDuYf"
   },
   "source": [
    "###  3.1 **`num_classes`**\n",
    "\n",
    "This parameter is used to set the number of classes according to the dataset that is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XLQZWAksMhO"
   },
   "outputs": [],
   "source": [
    "def update_num_classes(model_config, num_classes):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    model_config: model_pb2.DetectionModel object.\n",
    "    num_classes: `int` indicating the number of classes to set.\n",
    "    \"\"\"\n",
    "    \n",
    "    meta_architecture = model_config.WhichOneof(\"model\")\n",
    "\n",
    "    if meta_architecture == \"faster_rcnn\":\n",
    "        model_config.faster_rcnn.num_classes = num_classes\n",
    "    elif meta_architecture == \"ssd\":\n",
    "        model_config.ssd.num_classes = num_classes\n",
    "    elif meta_architecture == \"center_net\":\n",
    "        model_config.center_net.num_classes = num_classes\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T49fILhbDLIO"
   },
   "outputs": [],
   "source": [
    "# Total classes in your dataset \n",
    "num_classes = 4\n",
    "\n",
    "update_num_classes(configs['model'], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpKVgJgqEiAi"
   },
   "source": [
    "### 3.2  **`fine_tune_checkpoint`**\n",
    "\n",
    "We set this parameter to the path of our pre-trained model’s checkpoint so that we can use what it has learned to solve our problem and get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OdG6DKq6CTm"
   },
   "outputs": [],
   "source": [
    "def update_fine_tune_checkpoint_path(train_config, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    train_config: train_pb2.TrainConfig object.\n",
    "    checkpoint_path: path to pre-trained model’s checkpoint.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_config.fine_tune_checkpoint = checkpoint_path\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0T4wrslWEpH1"
   },
   "outputs": [],
   "source": [
    "fine_tune_checkpoint_path = os.path.join(model_directory, model_name, 'checkpoint', 'ckpt-0')\n",
    "\n",
    "update_fine_tune_checkpoint_path(configs['train_config'], fine_tune_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwqe8esdE_zC"
   },
   "source": [
    "3.3 **`num_steps`**\n",
    "\n",
    "`num_steps` are the number of total training steps for the model. In each step the model processes data samples that is equivalent to the batch size. So basically, `num_steps` is the total number of training iterations.\n",
    "\n",
    "**Note:** The `num_steps` is not equivalent to the number of epochs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi_7NQzxFCER"
   },
   "outputs": [],
   "source": [
    "num_steps = 10000\n",
    "\n",
    "config_util._update_train_steps(configs, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JANC4x3nFKp0"
   },
   "source": [
    "### 3.4 **`batch_size`**\n",
    "\n",
    "Ths is the number of training samples utilized in one training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndqtvVvLFOEy"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "config_util._update_batch_size(configs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdXxwh0nFoR1"
   },
   "source": [
    "### 3.5 **`train_input_reader`**\n",
    "\n",
    "The API has different input readers for taking training and testing data. The `train_input_reader` requires the path to our TF Records File which contains the data we need to use for training. So here we pass `train_input_config` to update the path to our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePN-grEly_YA"
   },
   "outputs": [],
   "source": [
    "train_record_path = 'train_data.tfrecord'\n",
    "\n",
    "config_util.update_input_reader_config(configs,\n",
    "                                       'train_input_config',\n",
    "                                       'tf_record_input_reader',\n",
    "                                       'input_path',\n",
    "                                        train_record_path,                                  \n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LtN4Y6rzRNa"
   },
   "source": [
    "Note that while updating the path to our tfrecord file we follow a hierarchy that is also followed in the `pipeline.config` file. And since we had to change the training data we use `train_input_config` in the second parameter. We will set a different name when updating the TFrecords path for validation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_0IqvA52fK0"
   },
   "source": [
    "### 3.6 **`eval_input_reader`**\n",
    "\n",
    "\n",
    "Here is another input reader. This one takes in validation data. Note that here we pass `eval_input_config` as one of the argument. Other than that it follows the same hierarchy as `train_input_reader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5txvc4DzTdx"
   },
   "outputs": [],
   "source": [
    "val_record_path = 'val_data.tfrecord'\n",
    "\n",
    "config_util.update_input_reader_config(configs,\n",
    "                                       'eval_input_config',\n",
    "                                       'tf_record_input_reader',\n",
    "                                       'input_path',\n",
    "                                       val_record_path,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpkQ6tbh2w1v"
   },
   "source": [
    "### 3.7 **`shuffle`**\n",
    "\n",
    "\n",
    "Set this param to `true` to randomize the dataset for better evaluation. We use this parameter to randomize the order of the data on which evaluation is performed at every step so we can better train and judge the model on its accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HH1hUxRx2uPl"
   },
   "outputs": [],
   "source": [
    "shuffle_flag = False\n",
    "\n",
    "config_util.update_input_reader_config(configs, \n",
    "                                       'eval_input_config',\n",
    "                                       None,\n",
    "                                       'shuffle',\n",
    "                                       shuffle_flag\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kdjeQox3or1"
   },
   "source": [
    "### 3.8 **`fine_tune_checkpoint_type`**\n",
    "\n",
    "We set this param to let our model know what kind of weights we want to use from the pre-trained model and what type of task we are going to train it for. In our case it's a detection task.\n",
    "\n",
    "You can know more about the other supported checkpoint types from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/train.proto#L43).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2t9uwrUx86kt"
   },
   "outputs": [],
   "source": [
    "def update_fine_tune_checkpoint_type(train_config, fine_tune_checkpoint_type):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    train_config: train_pb2.TrainConfig object.\n",
    "    fine_tune_checkpoint_type: determines the type of weights that are restored from \n",
    "                               from the pre-trained fine_tune_checkpoint.\n",
    "                               Can be either of: \"classification\", \"detection\" or \"full\".\n",
    "    \"\"\"\n",
    "    \n",
    "    train_config.fine_tune_checkpoint_type = fine_tune_checkpoint_type\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Wm0T6cT3kaR"
   },
   "outputs": [],
   "source": [
    "fine_tune_checkpoint_type = 'detection' \n",
    "\n",
    "update_fine_tune_checkpoint_type(configs['train_config'], fine_tune_checkpoint_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3fIy580zmh4"
   },
   "source": [
    "## 4 Label Map\n",
    "\n",
    "TensorFlow Object Detection API requires a label map which maps each of the labels to integer values. This labeled map is used at both times, during training and evaluation.\n",
    "\n",
    "Below we show an example label map (e.g `label_map.pbtxt`), assuming that our dataset contains 5 labels, `Ambulance`, `Bus`, `Car`, `Motorcycle` and `Truck`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHYgZhkE259s"
   },
   "outputs": [],
   "source": [
    "# Create Label Map of the Dataset\n",
    "pbtxt = '''\n",
    "item {\n",
    "    name: 'car',\n",
    "    id: 1,\n",
    "}\n",
    "\n",
    "item {\n",
    "    name: 'dog',\n",
    "    id: 2,\n",
    "}\n",
    "\n",
    "item {\n",
    "    name: 'person',\n",
    "    id: 3,\n",
    "}\n",
    "\n",
    "item {\n",
    "    name: 'tvmonitor',\n",
    "    id: 4,\n",
    "}\n",
    "'''\n",
    "\n",
    "# Save this labelmap to disk\n",
    "with open(\"labelmap.pbtxt\", \"w\") as text_file:\n",
    "    text_file.write(pbtxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwSC8dXM1U84"
   },
   "source": [
    "Label map files have the extension `.pbtxt`.\n",
    "\n",
    "**Note:** We start the label map from `1` because `0` is reserved for the background. Anything that is not part of the orginal classes is regarded as background and stored as `0`.\n",
    "\n",
    "Now we have to give this labelmap to our model's configuration file so our model is aware of the classes we are using and what their class ids are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xDzjhyruwoD"
   },
   "outputs": [],
   "source": [
    "labelmap_path = 'labelmap.pbtxt'\n",
    "\n",
    "config_util._update_label_map_path(configs, labelmap_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xr_gVrCF9GiC"
   },
   "source": [
    "## 5 Additional Params\n",
    "\n",
    "Beside's the above parameters there are lot's of other Additional `pipeline.config` parameters that you need not modify with `config_util` because they already have good default values but it's worth discussing a few of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG6o-KMduwoE"
   },
   "source": [
    "### 5.1 `data_augmentation_options`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBfD5fo2uwoE"
   },
   "source": [
    "Our config file has the following augmentation steps:\n",
    "\n",
    "```\n",
    "data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  \n",
    "data_augmentation_options {\n",
    "    random_crop_image {\n",
    "      min_object_covered: 0.0\n",
    "      min_aspect_ratio: 0.75\n",
    "      max_aspect_ratio: 3.0\n",
    "      min_area: 0.75\n",
    "      max_area: 1.0\n",
    "      overlap_thresh: 0.0\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "You can try out a variety of augmentations provided in the API. To see the list of augmentation techniques supported by TFOD API check [here](https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/protos/preprocessor.proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR3Y1Fcx_BbZ"
   },
   "source": [
    "Let's illustrate the explainations through some example. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_AlmnaY-7Gn"
   },
   "outputs": [],
   "source": [
    "from object_detection.protos import preprocessor_pb2 as preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrFG1ZlE_Lru"
   },
   "source": [
    "Suppose we want to add `random_adjust_contrast` as our augmentation step with the default values only. The following code cell can be used to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l68YrfZ0_F9a"
   },
   "outputs": [],
   "source": [
    "adj_contrast = preprocess.RandomAdjustContrast()\n",
    "\n",
    "preprocess_step = preprocess.PreprocessingStep()\n",
    "# Copy all the default values\n",
    "preprocess_step.random_adjust_contrast.CopyFrom(adj_contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKu3tepZ_b0r"
   },
   "source": [
    "Next, we append this preprocessing step to our `data_augmentation_options` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg3x-2Um_NWG"
   },
   "outputs": [],
   "source": [
    "configs['train_config'].data_augmentation_options.append(preprocess_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IysUaxB_fBq"
   },
   "source": [
    "Say, we want to update the parameters for a particular data augmentation manually. We can do this task, by executing the code cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zArS_RYg_RTA"
   },
   "outputs": [],
   "source": [
    "# Update the values manually\n",
    "preprocess_step.random_adjust_saturation.min_delta = 0.5\n",
    "preprocess_step.random_adjust_saturation.max_delta = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuTFp154_qPe"
   },
   "source": [
    "As discussed above, we will append it to our `data_augmentation_options` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKlqaQzv_XzT"
   },
   "outputs": [],
   "source": [
    "configs['train_config'].data_augmentation_options.append(preprocess_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZugEVLF_4Vd"
   },
   "source": [
    "Finally, let's take a look at all the data augmentations that have been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NYhC64G_0n4",
    "outputId": "e9d83a4b-9bbf-43bc-afe9-2108fdb2c7a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[random_horizontal_flip {\n",
       "}\n",
       ", random_crop_image {\n",
       "  min_object_covered: 0.0\n",
       "  min_aspect_ratio: 0.75\n",
       "  max_aspect_ratio: 3.0\n",
       "  min_area: 0.75\n",
       "  max_area: 1.0\n",
       "  overlap_thresh: 0.0\n",
       "}\n",
       ", random_adjust_contrast {\n",
       "}\n",
       ", random_adjust_saturation {\n",
       "  min_delta: 0.5\n",
       "  max_delta: 1.0\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs['train_config'].data_augmentation_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViYduiPFuwoE"
   },
   "source": [
    "### 5.2  **`image_resizer`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NcHniBdHzK4"
   },
   "source": [
    "Every model’s config file will come with a set image size which all the images will be resized to during training.\n",
    "\n",
    "Syntax from config file:\n",
    "\n",
    "```\n",
    "image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 640\n",
    "        width: 640\n",
    "      }\n",
    "    }\n",
    "```\n",
    "Generally the config files comes with 2 types of `images_resizer`:\n",
    "\n",
    "- `fixed_shape_resizer` : reshapes the images to a fixed size as specified in the `height` and `width` parameters\n",
    "\n",
    "\n",
    "- `keep_aspect_ratio_resizer`: defined by `min_dimension` and a `max_dimesion` for all images. These are scalar values. For e.g. if the `min_dimension` is a scalar value of `500` then during the training, the model will make sure that to resize the smaller dimension of an image while keeping the aspect ratio.\n",
    "\n",
    "According to the documentation provided in the [resize_to_range](https://github.com/tensorflow/models/blob/238922e98dd0e8254b5c0921b241a1f5a151782f/research/object_detection/core/preprocessor.py#L2895) function for resizing, it states:\n",
    "\n",
    "```\n",
    "The output size can be described by two cases:\n",
    "  1. If the image can be rescaled so its minimum dimension is equal to the\n",
    "     provided value without the other dimension exceeding max_dimension,\n",
    "     then do so.\n",
    "  2. Otherwise, resize so the largest dimension is equal to max_dimension.\n",
    "```\n",
    "\n",
    "Additionally we have a parameter `pad_to_max_dimension` for configurations with `keep_aspect_ratio_resizer` that resizes the image and pad it with zeros so the resulting image is of the spatial size `[max_dimension, max_dimension]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1aARahOH1c5",
    "outputId": "ada28e8f-9d76-423a-ca38-b3f0b13a37ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed_shape_resizer {\n",
       "  height: 640\n",
       "  width: 640\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use this to check the input size details.\n",
    "config_util.get_image_resizer_config(configs['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTcd_3tlIEHm"
   },
   "source": [
    "Now, let's update the image dimensions for the model configuration. We will subsequently call the `update_image_resizer`  function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zO8pzzkEH9mr"
   },
   "outputs": [],
   "source": [
    "def update_image_resizer(model_config, min_dim = None, max_dim = None, height = None, width = None):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    model_config: model_pb2.DetectionModel object.\n",
    "    min_dim: `int` ---> Desired size of the smaller image dimension in pixels.\n",
    "    max_dim: `int` ---> Desired size of the smaller image dimension in pixels.\n",
    "    height:  `int` ---> Desired height of image in pixels.\n",
    "    width:   `int` ---> Desired width of image in pixels.\n",
    "    \"\"\"\n",
    "    \n",
    "    meta_architecture = model_config.WhichOneof(\"model\")\n",
    "    print('Model Architecture: ', meta_architecture)\n",
    "    model = getattr(model_config,meta_architecture)\n",
    "    if model.image_resizer.HasField('fixed_shape_resizer'):\n",
    "        model.image_resizer.fixed_shape_resizer.height = height\n",
    "        model.image_resizer.fixed_shape_resizer.width = width\n",
    "            \n",
    "    elif model.image_resizer.HasField('keep_aspect_ratio_resizer'):\n",
    "        model.image_resizer.keep_aspect_ratio_resizer.min_dimension = min_dim \n",
    "        model.image_resizer.keep_aspect_ratio_resizer.max_dimension = max_dim\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0D--yBPlH-X4",
    "outputId": "8fd32c34-e82a-49d9-a1c2-b781fb2c8e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:  ssd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fixed_shape_resizer {\n",
       "  height: 780\n",
       "  width: 900\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_image_resizer(configs['model'], height = 780, width = 900 )\n",
    "config_util.get_image_resizer_config(configs['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UKUJPW3uwoF"
   },
   "source": [
    "### 5.3 Classification Loss and Localization Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8Zpmlu94Sbq"
   },
   "source": [
    "In Object Detection, we use two heads at the end of the network. One is for prediction of bounding boxes and one is for classification. Learning both of these requires the network to have two different losses for each of the heads. So we have a classification loss and a localization loss. \n",
    "\n",
    "There is also a weight we assign to both losses and by default in the TFOD API it is set to 1.0 for both of them because we want to give them equal weightage(importance) during training; such that the total loss is calculated in terms of: `localization_weight * localization_loss + classification_weight * classification loss` (along with `regularization_loss` if enabled)\n",
    "\n",
    "\n",
    "```\n",
    "loss {\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      classification_loss {\n",
    "        weighted_sigmoid_focal {\n",
    "          gamma: 2.0\n",
    "          alpha: 0.25\n",
    "        }\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "```\n",
    "\n",
    "You can take a look at the other losses that are available in the [losses.proto](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/losses.proto). However, it is not recommended to change the losses for the already pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8PrbAEuUVnC"
   },
   "source": [
    "Now, let's update the localization and classification weights for our model configuration. We will use the `update_loc_cls_wts` function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdXDaVQuUQgL"
   },
   "outputs": [],
   "source": [
    "def update_loc_cls_wts(model_config, loc_weight=1.0, cls_weight=1.0):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    model_config: model_pb2.DetectionModel object.\n",
    "    loc_weight: `float` ---> Localization loss weight.\n",
    "    cls_weight: `float` ---> Classification loss weight.\n",
    "    \"\"\"\n",
    "    \n",
    "    meta_architecture = model_config.WhichOneof(\"model\")\n",
    "\n",
    "    if meta_architecture == \"faster_rcnn\":\n",
    "        model_config.faster_rcnn.second_stage_localization_loss_weight = loc_weight\n",
    "        model_config.faster_rcnn.second_stage_classification_loss_weight = cls_weight\n",
    "        \n",
    "        \n",
    "    elif meta_architecture == \"ssd\":\n",
    "        model_config.ssd.loss.localization_weight = loc_weight\n",
    "        model_config.ssd.loss.classification_weight = cls_weight\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hz9EJbrMhxyJ"
   },
   "outputs": [],
   "source": [
    "update_loc_cls_wts(configs['model'], loc_weight=15.0, cls_weight=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79XC1hKlVNPp"
   },
   "source": [
    "Let's take a look at our updates for the localization and classification weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpbz8T16VDMu",
    "outputId": "78ed1f3c-2a45-473c-fd8a-64fc1463e0c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "localization_loss {\n",
       "  weighted_smooth_l1 {\n",
       "  }\n",
       "}\n",
       "classification_loss {\n",
       "  weighted_sigmoid_focal {\n",
       "    gamma: 2.0\n",
       "    alpha: 0.25\n",
       "  }\n",
       "}\n",
       "classification_weight: 1.5\n",
       "localization_weight: 15.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs['model'].ssd.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYmINHfy4au2"
   },
   "source": [
    "### 5.4 Learning Rate\n",
    "\n",
    "The learning rate is a hyperparameter that controls how much to tune the model in response to the prediction error each time the model weights are modified on a single training step.\n",
    "\n",
    "This is a really important parameter that decides how fast your model learns, failing to set this parameter properly can give you a lot of trouble.\n",
    "\n",
    "**TFOD API** employs some complex learning schemes to select and change the learning rate througout the training. \n",
    "\n",
    "\n",
    "This includes a warm learning rate, warm up steps, the actual learning rate, a learning_rate_decay etc. \n",
    "\n",
    "```\n",
    "optimizer {\n",
    "    momentum_optimizer {\n",
    "      learning_rate {\n",
    "        cosine_decay_learning_rate {\n",
    "          learning_rate_base: 0.04\n",
    "          total_steps: 25000\n",
    "          warmup_learning_rate: 0.013333\n",
    "          warmup_steps: 2000\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.9\n",
    "    }\n",
    "    use_moving_average: false\n",
    "  }\n",
    "```\n",
    "\n",
    "Lets go over the additional parameters within learning rate:\n",
    "\n",
    "* **Base Learning Rate**\n",
    "    This value is the rate at which the model learns normally for the whole training process if a **warm up learning rate** is not set.\n",
    "  \n",
    "* **Warm Up Learning Rate**\n",
    "    Whenever we start the model for training, it is often a good practice to initially let the model learn slowly so it can learn from the early examples correctly. After the model has learned for a set number of steps. We change the learning rate to a value which lets our model learn and reach convergance faster.\n",
    "\n",
    "* **Total Steps**\n",
    "    This parameter is the same as the the same as `num_steps` parameter we studied above. \n",
    "\n",
    "* **Warmup Steps**\n",
    "    These are the number of steps during which the learning rate starts from the `warmup_learning_rate` and goes till the `learning_rate_base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LQrVH6YVfjZ"
   },
   "outputs": [],
   "source": [
    "# Initial LR\n",
    "initial_lr = 0.0003\n",
    "\n",
    "# Update Initial LR\n",
    "config_util._update_initial_learning_rate(configs, initial_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtAZgfeS9zew"
   },
   "source": [
    "### 5.5 Feature Extractor\n",
    "\n",
    "A pre-trained model is a network that was previously trained on the MS COCO 2017. You either use the pretrained model as is or use transfer learning to customize this model for a given task. \n",
    "\n",
    "The feature extractor is the name of the pretrained model that is being used for fine_tuning. This is also referred as the back bone model or the base model.\n",
    "\n",
    "It useful to check what backbone is being used for your detection task.\n",
    "\n",
    "For instance, if we are using a pre-trained **RetinaNet101**, the backbone is a `ssd_resnet101_v1_fpn_keras`. \n",
    "\n",
    "```\n",
    "feature_extractor {\n",
    "      type: \"ssd_resnet101_v1_fpn_keras\"\n",
    "      depth_multiplier: 1.0\n",
    "      min_depth: 16\n",
    "      conv_hyperparams {\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 0.0004\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          truncated_normal_initializer {\n",
    "            mean: 0.0\n",
    "            stddev: 0.03\n",
    "          }\n",
    "        }\n",
    "        activation: RELU_6\n",
    "        batch_norm {\n",
    "          decay: 0.997\n",
    "          scale: true\n",
    "          epsilon: 0.001\n",
    "        }\n",
    "      }\n",
    "      override_base_feature_extractor_hyperparams: true\n",
    "      fpn {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "      }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpY989CUuwoG"
   },
   "source": [
    "### 5.6 **`iou_threshold`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dC_n1IHz-lCs"
   },
   "source": [
    "In **Non-max Suppression**, we need `iou_threshold` to suppress less confident predictions in the case of less confident bounding box overlaps sufficiently with a more confident bounding box prediction. \n",
    "\n",
    "```\n",
    "post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 1e-08\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "        use_static_shapes: false\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjl1W4fuW1gQ"
   },
   "outputs": [],
   "source": [
    "def update_iou_threshold(model_config, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    model_config: model_pb2.DetectionModel object.\n",
    "    iou_thresh: `float` ---> IoU threshold required during Non-Maximum suppression.\n",
    "    \"\"\"\n",
    "    \n",
    "    meta_architecture = model_config.WhichOneof(\"model\")\n",
    "\n",
    "    if meta_architecture == \"faster_rcnn\":\n",
    "        model_config.faster_rcnn.second_stage_post_processing.batch_non_max_suppression.iou_threshold = iou_thresh\n",
    "        \n",
    "        \n",
    "    elif meta_architecture == \"ssd\":\n",
    "        model_config.ssd.post_processing.batch_non_max_suppression.iou_threshold = iou_thresh        \n",
    "  \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qBcqzkdh8Vn"
   },
   "outputs": [],
   "source": [
    "update_iou_threshold(configs['model'], iou_thresh=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXlQyqLMDxW8"
   },
   "source": [
    "### 5.7 **`max_detections_per_class`**\n",
    "\n",
    "This parameter allows you to set the maximum number of detections of a single class on an image. \n",
    "\n",
    "**e.g:** maximum 100 cats should be detected on each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1ak_9mDiWXa"
   },
   "outputs": [],
   "source": [
    "def update_max_detections_per_class(model_config, max_dets_per_class=100):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    model_config: model_pb2.DetectionModel object.\n",
    "    max_dets_per_class: `int` ---> Maximum number of detections to retain per class.\n",
    "    \"\"\"\n",
    "    \n",
    "    meta_architecture = model_config.WhichOneof(\"model\")\n",
    "\n",
    "    if meta_architecture == \"faster_rcnn\":\n",
    "        model_config.faster_rcnn.second_stage_post_processing.batch_non_max_suppression.max_detections_per_class = max_dets_per_class\n",
    "        \n",
    "        \n",
    "    elif meta_architecture == \"ssd\":\n",
    "        model_config.ssd.post_processing.batch_non_max_suppression.max_detections_per_class = max_dets_per_class    \n",
    "  \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrU8fhNgi3fU"
   },
   "outputs": [],
   "source": [
    "update_max_detections_per_class(configs['model'], max_dets_per_class=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pD5d1LI_CSW"
   },
   "source": [
    "### 5.8 **`max_total_detections`**\n",
    "\n",
    "This parameter sets the maximum number of total detections which is the sum of all detections of all classes combined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qso139Hhi8fM"
   },
   "outputs": [],
   "source": [
    "def update_max_total_detections(model_config, max_dets=100):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    model_config: model_pb2.DetectionModel object.\n",
    "    max_dets: `int` ---> Maximum number of detections to retain across all classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    meta_architecture = model_config.WhichOneof(\"model\")\n",
    "\n",
    "    if meta_architecture == \"faster_rcnn\":\n",
    "        model_config.faster_rcnn.second_stage_post_processing.batch_non_max_suppression.max_total_detections = max_dets\n",
    "        \n",
    "        \n",
    "    elif meta_architecture == \"ssd\":\n",
    "        model_config.ssd.post_processing.batch_non_max_suppression.max_total_detections = max_dets\n",
    "  \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Yq5VchRi-qt"
   },
   "outputs": [],
   "source": [
    "update_max_total_detections(configs['model'], max_dets=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zj6IQi8W83NK"
   },
   "source": [
    "## 6 Save the edited Config File\n",
    "\n",
    "After you're done making changing then it's worth mentioning that all the changes are made in memory and the actual file and you'll need to serialize it in a file. So first we have to create a config file containing those changes using `config_util.create`\n",
    "\n",
    "And then we'll serialize this and with `config_util.save`, this will generate a new `pipeline.config` file on disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jT-esoAQl-1q"
   },
   "outputs": [],
   "source": [
    "# Create a pipeline file instance from the edited configuration instance\n",
    "configs_file = config_util.create_pipeline_proto_from_configs(configs)\n",
    "\n",
    "# Save the pipeline into a directory\n",
    "config_util.save_pipeline_config(configs_file, './')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sF-kK9XQMXb"
   },
   "source": [
    "## 7 Conclusion\n",
    "\n",
    "In this notebook, we have learned the following with respect to the configuration parameters:\n",
    "\n",
    "1. Modify the parameters with respect to the dataset for which we are training our model on such as: `num_classes`, `batch_size`, labelmap, etc.\n",
    "\n",
    "\n",
    "2. Try out a variety of pretrained model checkpoints available from [TensorFlow 2 Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).\n",
    "\n",
    "\n",
    "3. Explore additional parameters such as dat augmentations, `image_resizer`, etc.\n",
    "\n",
    "\n",
    "4. Learn about model hyperparameters such as loss functions, learning rate optimizers, etc."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "c4_06_30_Pipeline_Configuration_TFOD.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
